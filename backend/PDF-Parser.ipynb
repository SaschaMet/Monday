{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from qdrant_client import QdrantClient, models\n",
    "from qdrant_client.models import Distance, VectorParams\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import cohere\n",
    "import uuid\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DESKTOP_PATH = \"/Users/saschametzger/Desktop/\"\n",
    "FILE_PATH = DESKTOP_PATH + \"phi3.pdf\"\n",
    "\n",
    "COLLECTION_NAME = \"documents\"\n",
    "\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "META_DATA = {\n",
    "    \"title\": \"phi3\",\n",
    "}\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PyPDFLoader(FILE_PATH)\n",
    "pages = loader.load_and_split()\n",
    "len(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant = QdrantClient(url=\"http://localhost:6333\")\n",
    "co = cohere.Client(os.getenv(\"COHERE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant.delete_collection(COLLECTION_NAME)\n",
    "\n",
    "qdrant.recreate_collection(\n",
    "    COLLECTION_NAME,\n",
    "    vectors_config=VectorParams(\n",
    "        size=1024,\n",
    "        distance=Distance.COSINE,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_splitter = SemanticChunker(\n",
    "#     FastEmbedEmbeddings(\n",
    "#         model_name=EMBEDDING_MODEL,\n",
    "#     ),\n",
    "#     breakpoint_threshold_type=\"percentile\",\n",
    "# )\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = []\n",
    "metadata = []\n",
    "uuids = []\n",
    "\n",
    "for i, page in enumerate(pages):\n",
    "    for chunk in text_splitter.split_text(page.page_content):\n",
    "        content.append(chunk)\n",
    "        metadata.append(\n",
    "            {\n",
    "                **META_DATA,\n",
    "                \"page\": str(i + 1),\n",
    "                \"content\": chunk,\n",
    "            }\n",
    "        )\n",
    "        uuids.append(str(uuid.uuid4()))\n",
    "\n",
    "len(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = co.embed(\n",
    "    texts=content,\n",
    "    model=\"embed-multilingual-v3.0\",\n",
    "    input_type=\"search_document\",\n",
    ").embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(content) == len(metadata) == len(uuids) == len(embeddings)\n",
    "assert len(content) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings.embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:00<00:00, 212.13it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, _ in enumerate(tqdm(range(len(embeddings)))):\n",
    "    qdrant.upsert(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        points=[\n",
    "            models.PointStruct(\n",
    "                id=uuids[i],\n",
    "                payload={\n",
    "                    **metadata[i],\n",
    "                },\n",
    "                vector=embeddings[i],\n",
    "            ),\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the context length of phi-3?\"\n",
    "\n",
    "query_vector = co.embed(\n",
    "    texts=content,\n",
    "    model=\"embed-multilingual-v3.0\",\n",
    "    input_type=\"search_query\",\n",
    ").embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='f4d362eb-66bb-4289-8a73-18fc5a65eafb', version=0, score=0.9058182, payload={'content': 'Phi-3 Technical Report:\\nA Highly Capable Language Model Locally on Your Phone\\nMicrosoft\\nAbstract\\nWe introduce phi-3-mini , a 3.8 billion parameter language model trained on 3.3 trillion tokens,\\nwhose overall performance, as measured by both academic benchmarks and internal testing, rivals\\nthat of models such as Mixtral 8x7B and GPT-3.5 (e.g., phi-3-mini achieves 69% on MMLU and 8.38\\non MT-bench), despite being small enough to be deployed on a phone. The innovation lies entirely in\\nour dataset for training, a scaled-up version of the one used for phi-2 , composed of heavily filtered\\nweb data and synthetic data. The model is also further aligned for robustness, safety, and chat format.\\nWe also provide some initial parameter-scaling results with a 7B and 14B models trained for 4.8T\\ntokens, called phi-3-small andphi-3-medium , both significantly more capable than phi-3-mini\\n(e.g., respectively 75% and 78% on MMLU, and 8.7 and 8.9 on MT-bench).\\n1 Introduction\\nThe striking progress of AI in the last few years can be largely attributed to major efforts through-\\nout the world towards scaling-up to ever-larger models and datasets. Large Language Models (LLMs)\\nhave steadily increased in size from a mere billion parameters just five years ago (GPT-2 had 1.5 bil-\\nlion parameters [RWC+19]) to trillion parameters today. The impetus for this effort originates in the\\nseemingly predictable improvement one obtains by training large models, the so-called scaling laws\\n[KMH+20, HBM+22, MRB+23]. However these laws assume a “fixed” data source. This assumption\\nis now significantly disrupted by the existence of frontier LLMs themselves, which allow us to interact\\nwith data in novel ways. In our previous works on the phi models [GZA+23, LBE+23, JBA+23] it was\\nshown that a combination of LLM-based filtering of web data, and LLM-created synthetic data, enable\\nperformance in smaller language models that were typically seen only in much larger models. For exam-\\nple our previous model trained on this data recipe, phi-2 (2.7B parameters), matched the performance\\nof models 25 times larger trained on regular data. In this report we present a new model, phi-3-mini\\n(3.8B parameters), trained for 3.3T tokens on larger and more advanced versions of the datasets used in\\nphi-2 . With its small size, phi-3-mini can easily be inferenced locally on a modern phone (see Figure\\n1), yet it achieves a quality that seems on-par with models such as Mixtral 8x7B [JSR+24] and GPT-3.5.\\nUser: Explain why it is surprising that one can build a language model small enough to\\nfit on a phone, yet almost as powerful as ChatGPT. Just use one funny sentence.\\nphi-3-mini: It’s like fitting a supercomputer in a flip phone, but instead of breaking\\nthe phone, it just breaks the internet with its tiny, yet mighty, linguistic prowess!\\nUser: Okay now more serious answer, and note that this was achieved solely by changing\\nthe training data.\\nphi-3-mini: The development of a compact language model that rivals the capabilities\\nof ChatGPT, while fitting on a phone, is a testament to the power of data-driven\\nmachine learning. By meticulously curating and optimizing the training dataset,\\nresearchers can significantly reduce the model’s size without compromising its\\nperformance. [...]\\n1arXiv:2404.14219v1  [cs.CL]  22 Apr 2024', 'page': '1', 'title': 'phi3'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='66e7ae3b-1bb6-4de7-b6c0-4a493e63766d', version=1, score=0.76233643, payload={'content': '2 Technical Specifications\\nThephi-3-mini model is a transformer decoder architecture [VSP+17], with default context length 4 K.\\nWe also introduce a long context version via LongRope [DZZ+24] that extends the context length to\\n128K, called phi-3-mini-128K .\\nTo best benefit the open source community, phi-3-mini is built upon a similar block structure as\\nLlama-2 [TLI+23] and uses the same tokenizer with vocabulary size of 320641. This means that all\\npackages developed for Llama-2 family of models can be directly adapted to phi-3-mini . The model\\nuses 3072 hidden dimension, 32 heads and 32 layers. We trained using bfloat16 for a total of 3.3T tokens.\\nThe model is already chat-finetuned, and the chat template is as follows:\\n<|user|> /n Question <|end|> /n <|assistant|>\\nThephi-3-small model (7B parameters) leverages the tiktoken tokenizer (for better multilingual to-\\nkenization) with a vocabulary size of 100352 and has default context length 8 K. It follows the standard\\ndecoder architecture of a 7B model class, having 32 layers and a hidden size of 4096. To minimize KV\\ncache footprint, the model also leverages a grouped-query attention, with 4 queries sharing 1 key. More-\\noverphi-3-small uses alternative layers of dense attention and a novel blocksparse attention to further\\noptimize on KV cache savings while maintaining long context retrieval performance. An additional 10%\\nmultilingual data was also used for this model.\\nHighly capable language model running locally on a cell-phone. Thanks to its small size, phi-\\n3-mini can be quantized to 4-bits so that it only occupies ≈1.8GB of memory. We tested the quantized\\nmodel by deploying phi-3-mini on iPhone 14 with A16 Bionic chip running natively on-device and fully\\noffline achieving more than 12 tokens per second.\\nTraining Methodology. We follow the sequence of works initiated in “Textbooks Are All You\\nNeed” [GZA+23], which utilize high quality training data to improve the performance of small language\\nmodels and deviate from the standard scaling-laws . In this work we show that such method allows to\\nreach the level of highly capable models such as GPT-3.5 or Mixtral with only 3.8B total parameters\\n(while Mixtral has 45B total parameters for example). Our training data of consists of heavily filtered\\nweb data (according to the “educational level”) from various open internet sources, as well as synthetic\\nLLM-generated data. Pre-training is performed in two disjoint and sequential phases; phase-1 comprises\\nmostly of web sources aimed at teaching the model general knowledge and language understanding.\\nPhase-2 merges even more heavily filtered webdata (a subset used in Phase-1) with some synthetic data\\nthat teach the model logical reasoning and various niche skills.\\nData Optimal Regime. Unlike prior works that train language models in either “compute optimal\\nregime” [HBM+22] or “over-train regime”, we mainly focus on the quality of data for a given scale .2\\nWe try to calibrate the training data to be closer to the “data optimal” regime for small models. In\\nparticular, we filter the web data to contain the correct level of “knowledge” and keep more web pages\\nthat could potentially improve the “reasoning ability” for the model. As an example, the result of a\\ngame in premier league in a particular day might be good training data for frontier models, but we need\\nto remove such information to leave more model capacity for “reasoning” for the mini size models. We\\ncompare our approach with Llama-2 in Figure 2.\\n1We remove BoS tokens and add some additional tokens for chat template.\\n2Just like for “compute optimal regime”, we use the term “optimal” in an aspirational sense for “data optimal regime”.\\nWe are not implying that we actually found the provably “optimal” data mixture for a given scale.\\n2', 'page': '2', 'title': 'phi3'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='f7ca8209-6119-46f9-a8ab-1e32a050a7a5', version=3, score=0.75175476, payload={'content': 'To test our data on larger size of models, we also trained phi-3-medium , a model with 14B pa-\\nrameters using the same tokenizer and architecture of phi-3-mini , and trained on the same data for\\nslightly more epochs (4.8T tokens total as for phi-3-small ). The model has 40 heads and 40 layers,\\nwith embedding dimension 5120. We observe that some benchmarks improve much less from 7B to 14B\\nthan they do from 3.8B to 7B, perhaps indicating that our data mixture needs further work to be in\\nthe “data optimal regime” for 14B parameters model. We are still actively investigating some of those\\nbenchmarks (including a regression on HumanEval), hence the numbers for phi-3-medium should be\\nconsidered as a “preview”.\\nPost-training. Our models went through post-training with both supervised instruction fine-tuning,\\nand preference tuning with DPO. We have worked on generating and curating various instruction and\\npreference data. This has improved the model chat capabilities, robustness, as well as its safety.\\n3 Academic benchmarks\\nOn the next page we report the results for phi-3-mini on standard open-source benchmarks measuring\\nthe model’s reasoning ability (both common sense reasoning and logical reasoning). We compare to phi-2\\n[JBA+23], Mistral-7b-v0.1 [JSM+23], Mixtral-8x7b [JSR+24], Gemma 7B [TMH+24], Llama-3-instruct-\\n8b [AI23], and GPT-3.5. All the reported numbers are produced with the exact same pipeline to ensure\\nthat the numbers are comparable. These numbers might differ from other published numbers due to\\nslightly different choices in the evaluation. As is now standard, we use few-shot prompts to evaluate\\nthe models, at temperature 0. The prompts and number of shots are part of a Microsoft internal tool\\nto evaluate language models, and in particular we did no optimization to the pipeline for the phi-3\\nmodels.3The number of k–shot examples is listed per-benchmark. An example of a 2-shot prompt is\\ndescribed in Appendix A.\\n3For example, we found that using ## before the Question can lead to a noticeable improvement to phi-3-mini ’s\\nresults across many benchmarks, but we did not do such changes in the prompts.\\n4', 'page': '4', 'title': 'phi3'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='86809270-6765-415d-9986-f35dac72dad8', version=6, score=0.7208721, payload={'content': 'Figure 4: Left: phi-3-mini ’s completion without search. Right: phi-3-mini ’s completion with search, using the\\ndefault HuggingFace Chat-UI search ability.\\n5 Weakness\\nIn terms of LLM capabilities, while phi-3-mini model achieves similar level of language understanding\\nand reasoning ability as much larger models, it is still fundamentally limited by its size for certain tasks.\\nThe model simply does not have the capacity to store too much “factual knowledge”, which can be seen\\nfor example with low performance on TriviaQA. However, we believe such weakness can be resolved by\\naugmentation with a search engine. We show an example using the HuggingFace default Chat-UI with\\nphi-3-mini in Figure 4. Another weakness related to model’s capacity is that we mostly restricted the\\nlanguage to English. Exploring multilingual capabilities for Small Language Models is an important\\nnext step, with some initial promising results on phi-3-small by including more multilingual data.\\nDespite our diligent RAI efforts, as with most LLMs, there remains challenges around factual inaccu-\\nracies (or hallucinations), reproduction or amplification of biases, inappropriate content generation, and\\nsafety issues. The use of carefully curated training data, and targeted post-training, and improvements\\nfrom red-teaming insights significantly mitigates these issues across all dimensions. However, there is\\nsignificant work ahead to fully address these challenges.\\n7', 'page': '7', 'title': 'phi3'}, vector=None, shard_key=None),\n",
       " ScoredPoint(id='d8d4b6e3-6332-47c8-8c38-b8cf6829a856', version=7, score=0.6323519, payload={'content': 'References\\n[AI23] Meta AI. Introducing meta llama 3: The most capable openly available llm to date, 2023.\\n[AON+21] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program\\nsynthesis with large language models. arXiv preprint arXiv:2108.07732 , 2021.\\n[BJN+22] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kada-\\nvath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds,\\nDanny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda,\\nCatherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah,\\nBen Mann, and Jared Kaplan. Training a helpful and harmless assistant with reinforcement\\nlearning from human feedback, 2022.\\n[BSA+24] Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R¨ ottger, Dan Jurafsky, Tatsunori\\nHashimoto, and James Zou. Safety-tuned llamas: Lessons from improving the safety of large\\nlanguage models that follow instructions, 2024.\\n[BZGC19] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical\\ncommonsense in natural language. arXiv preprint arXiv:1911.11641 , 2019.\\n[CCE+18] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\\nchallenge, 2018.\\n[CKB+21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher\\nHesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint\\narXiv:2110.14168 , 2021.\\n[CLC+19] Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\\nKristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions.\\nInProceedings of the 2019 Conference of the North American Chapter of the Association\\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short\\nPapers) , pages 2924–2936, 2019.\\n[CTJ+21] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\\nBrooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo-\\nhammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings,\\nMatthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen\\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji,\\nShantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh\\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage,\\nMira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish,\\nIlya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code,\\n2021.\\n8', 'page': '8', 'title': 'phi3'}, vector=None, shard_key=None)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant.search(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    query_vector=query_vector,\n",
    "    limit=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
